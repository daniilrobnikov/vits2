{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split filelist file into train and test sets\n",
        "\n",
        "Use a train ratio or number of samples in test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/daniilrobnikov/Developer/TTS/vits-bengali\n",
            "LICENSE                         \u001b[34mpreprocess\u001b[m\u001b[m/\n",
            "README.md                       preprocess.py\n",
            "attentions.py                   requirements.txt\n",
            "batch_inference.ipynb           \u001b[34mresources\u001b[m\u001b[m/\n",
            "commons.py                      test-env.md\n",
            "\u001b[34mconfigs\u001b[m\u001b[m/                        test-gpu_monotonic_align.ipynb\n",
            "data_utils.py                   test-madasr23-links.txt\n",
            "\u001b[34mfilelists\u001b[m\u001b[m/                      test-todo.txt\n",
            "inference.ipynb                 test_torchaudio.ipynb\n",
            "losses.py                       \u001b[34mtext\u001b[m\u001b[m/\n",
            "mel_processing.py               train.py\n",
            "models.py                       train_ms.py\n",
            "modules.py                      transforms.py\n",
            "monotonic_align.py              utils.py\n"
          ]
        }
      ],
      "source": [
        "# Load the data from the csv file\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "dataset_name = \"madasr23\"\n",
        "data: pd.DataFrame = pd.read_csv(f\"../filelists/{dataset_name}.csv\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Support for DataFrames\n",
        "def split_file_list(orig_data: pd.DataFrame, train_ratio=None, test_samples=None, max_samples=None):\n",
        "    # Shuffle the data\n",
        "    data = orig_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    if max_samples is not None:\n",
        "        data = data[:max_samples]\n",
        "\n",
        "    if test_samples is not None:\n",
        "        train_set = data[:-test_samples]\n",
        "        test_set = data[-test_samples:]\n",
        "    elif train_ratio is not None:\n",
        "        train_set_size = int(len(data) * train_ratio)\n",
        "        train_set = data[:train_set_size]\n",
        "        test_set = data[train_set_size:]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Either 'train_ratio' or 'test_samples' should be provided.\")\n",
        "\n",
        "    return train_set, test_set\n",
        "\n",
        "\n",
        "# Example usage\n",
        "train_data, val_data = split_file_list(data, test_samples=240)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save phonemes and text of train_data, val_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i_dir = \"path/to/wav/directory\"\n",
        "o_file_train = f\"../filelists/{dataset_name}_audio_sid_text_train_filelist.txt\"\n",
        "o_file_val = f\"../filelists/{dataset_name}_audio_sid_text_test_filelist.txt\"\n",
        "\n",
        "link_name = \"DUMMY3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_path_map(source_dir):\n",
        "    path_map = {}\n",
        "    for root, dirs, files in os.walk(source_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                path_map[file] = os.path.join(root, file)\n",
        "    return path_map\n",
        "\n",
        "\n",
        "def save_file_list(data, out_file_path, source_dir, path_map, link_name, cleaned_text=False):\n",
        "    with open(out_file_path, \"w\") as file:\n",
        "        for row in data.itertuples():\n",
        "            uttid = f\"{row.uttid}.wav\"\n",
        "            path = path_map[uttid].replace(source_dir, link_name)\n",
        "            spkidx = row.spkidx\n",
        "            info = row.text if not cleaned_text else row.phonemes\n",
        "\n",
        "            file.write(f\"{path}|{spkidx}|{info}\\n\")\n",
        "            # Print every nth sample\n",
        "            if row.Index % 5000 == 0:\n",
        "                print(f\"{row.Index}: {path}|{spkidx}|{info}\")\n",
        "\n",
        "    print(f\"Saved to '{out_file_path}' ({len(data)} samples).\")\n",
        "\n",
        "\n",
        "def save_files(data, out_file_path, source_dir, path_map, link_name):\n",
        "    save_file_list(train_data, out_file_path, source_dir, path_map, link_name)\n",
        "    if \"phonemes\" in data.columns:\n",
        "        out_file_path = out_file_path.replace(\".txt\", \".txt.cleaned\")\n",
        "        save_file_list(data, out_file_path, source_dir,\n",
        "                       path_map, link_name, cleaned_text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_map = create_path_map(i_dir)\n",
        "\n",
        "\n",
        "save_files(train_data, o_file_train, i_dir, path_map, link_name)\n",
        "save_files(val_data, o_file_val, i_dir, path_map, link_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a symlink to the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create symlink to the dataset\n",
        "!ln -s {i_dir} {link_name}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
